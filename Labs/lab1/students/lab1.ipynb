{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验一：从零开始实现基于Transformer的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目标\n",
    "\n",
    "在本次试验中，你将实现一个Decoder-only的Transformer模型：GPT。你将使用实验提供的中文语料训练这个GPT模型。\n",
    "\n",
    "我们在此notebook中提供了训练一个Transformer模型的基础代码框架。请你将未实现的功能补充完整。\n",
    "\n",
    "参考资料：\n",
    "- 课件\n",
    "- Transformer原文：[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- GPT-1原文：[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "- 参考文章：transformer详解 https://zhuanlan.zhihu.com/p/338817680\n",
    "- 参考文章：PyTorch从零开始实现Transformer https://blog.csdn.net/shizheng_Li/article/details/131721198?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522171119446616800184118303%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=171119446616800184118303&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-131721198-null-null.142^v99^pc_search_result_base4&utm_term=TRANSFORMER&spm=1018.2226.3001.4187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请使用实验提供的`requirements.txt`配置环境，推荐使用python 3.10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:01:49.552577Z",
     "start_time": "2023-11-10T11:01:49.509808Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from tqdm.auto import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "seed = 1228\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型输入\n",
    "\n",
    "模型无法直接理解文本，因此需要先将文本进行子词切分（tokenize），即根据词表把各个文本token转换为对应的整数id。请你运行下面的单元格，初步尝试使用cl100k-base版本的tokenizer进行子词切分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:01:49.553296Z",
     "start_time": "2023-11-10T11:01:49.515179Z"
    }
   },
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "text = \"你好，我在学习NLP\"\n",
    "enc.encode_ordinary(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次试验的数据集为天工发布的SkyPile数据集的子集，位于`./data`中。为了提高训练效率，我们需要先对原始数据集进行tokenize，请你在下方的单元格中运行`./data/prepare.py`，将字符串格式的数据集转为numpy整数格式，并切分训练集和验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer (Decoder-only) 的实现 (15分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆Transformer模型的结构（如下图）：\n",
    "\n",
    "![](./img/transformer.png)\n",
    "\n",
    "Decoder部分位于图中的右半侧，包括：Embedding、Positional Encoding、Masked Multi-Head Attention、Layernorm、Feed Forward Network，下面将分步实现各个部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型配置\n",
    "\n",
    "我们将配置模型的参数放入`GPTConfig`中，便于后续调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:01:49.906694Z",
     "start_time": "2023-11-10T11:01:49.904306Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256  # 模型最大序列长度\n",
    "    vocab_size: int = enc.n_vocab  # 词表大小\n",
    "    n_layer: int = 6  # 模型层数\n",
    "    n_head: int = 6  # 多头注意力的头数\n",
    "    n_embd: int = 384  # 模型隐层维度\n",
    "    dropout: float = 0.2  # 模型 dropout 比率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Token Embedding (1分)\n",
    "\n",
    "当输入id进入模型后，模型需要使用Embedding将离散的id转换为向量。\n",
    "\n",
    "请你尝试使用`nn.Embedding`将下方的id转换为向量，并通过assert测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:03:46.807423Z",
     "start_time": "2023-11-10T11:03:46.611428Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_len = 2\n",
    "vocab_size = 4\n",
    "n_embd = 5\n",
    "\n",
    "input_ids = torch.arange(seq_len, dtype=torch.long)  # shape = [T]\n",
    "\n",
    "# ==============================================================================\n",
    "# ===== 实现以下部分 (1分)\n",
    "# =====     - 将 shape = [T] 的输入转为 shape = [T, n_embd] 的向量 (1分)\n",
    "# ==============================================================================\n",
    "wte = TODO\n",
    "# ==============================================================================\n",
    "# ===== 实现以上部分\n",
    "# ==============================================================================\n",
    "\n",
    "output = wte(input_ids)  # shape = [T, n_embd]\n",
    "print(output)\n",
    "\n",
    "assert output.shape == (input_ids.shape[0], n_embd), \"Wrong implementation\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(input_ids.view(1, seq_len).numpy())\n",
    "plt.title(\"input ids\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"seq len\")\n",
    "plt.ylabel(\"id\")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(torch.nn.functional.one_hot(input_ids, num_classes=vocab_size).numpy())\n",
    "plt.title(\"input ids (one hot)\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"vocab size\")\n",
    "plt.ylabel(\"seq len\")\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(wte.weight.detach().numpy())\n",
    "plt.title(\"Embedding\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"n_embd\")\n",
    "plt.ylabel(\"vocab size\")\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(output.detach().numpy())\n",
    "plt.title(\"output = \\none hot * Embedding\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"n_embd\")\n",
    "plt.ylabel(\"seq len\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding可以将token id转为向量。由于模型内部在进行多次运算后的结果还是一组向量，为了能正常生成文字，模型还需要一种能将向量转为token id的组件。这就是lm head，只需要将onehot和Embedding的功能倒转即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding (1分)\n",
    "\n",
    "GPT的位置编码与Embedding类似，是可学习的，这一点与Transformer原文中的`sin/cos`位置编码不同。\n",
    "\n",
    "请你尝试使用`nn.Embedding`实现GPT的位置编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ===== 实现以下部分 (1分)\n",
    "# ==============================================================================\n",
    "wpe = TODO\n",
    "# ==============================================================================\n",
    "# ===== 实现以上部分\n",
    "# ==============================================================================\n",
    "\n",
    "pos = torch.arange(0, seq_len, dtype=torch.long)\n",
    "\n",
    "tok_emb = wte(input_ids)  # shape = [T, n_embd]\n",
    "pos_emb = wpe(pos)  # shape = [T, n_embd]\n",
    "output = tok_emb + pos_emb\n",
    "print(output)\n",
    "assert output.shape == (input_ids.shape[0], n_embd), \"Wrong implementation\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(tok_emb.detach().numpy())\n",
    "plt.title(\"tok emb\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"n_embd\")\n",
    "plt.ylabel(\"vocab size\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(pos_emb.detach().numpy())\n",
    "plt.title(\"pos emb\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"n_embd\")\n",
    "plt.ylabel(\"seq len\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(output.detach().numpy())\n",
    "plt.title(\"output = tok + pos\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"n_embd\")\n",
    "plt.ylabel(\"seq len\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Masked Multi-Head Attention (4 + 2分)\n",
    "\n",
    "下面需要你实现GPT模型的多头自注意力。\n",
    "\n",
    "在Decoder-only模型训练时，模型的输入是一个完整的序列，而在实际推理时，模型的输入是自回归的。因此需要保证当前token只能“看到”以前的token，而看不到以后的token，以模拟实际推理时的情景。\n",
    "\n",
    "![](./img/attention.png)\n",
    "\n",
    "这需要你实现一个注意力掩码，来屏蔽当前token对后边token的注意力。\n",
    "\n",
    "> 提示：由于注意力分数需要经过softmax计算，因此只需要将某些位置的值设置为负无穷，经过softmax后相应位置的值就会是0。\n",
    "\n",
    "请你尝试用`torch.tril`构造一个上三角矩阵来实现注意力掩码 (4分)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "n_embd = 16\n",
    "\n",
    "q = torch.rand(seq_len, n_embd)\n",
    "k = torch.rand(seq_len, n_embd)\n",
    "v = torch.rand(seq_len, n_embd)\n",
    "\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "att_unmask = att.clone()\n",
    "\n",
    "# ==============================================================================\n",
    "# ===== 实现以下部分 (4分)\n",
    "# =====     - 使用`torch.tril`构造三角矩阵\n",
    "# =====     - 使用`-inf`进行注意力屏蔽\n",
    "# ==============================================================================\n",
    "mask = TODO\n",
    "att = TODO\n",
    "# ==============================================================================\n",
    "# ===== 实现以上部分\n",
    "# ==============================================================================\n",
    "att_masked = att.clone()\n",
    "att = F.softmax(att, dim=-1)\n",
    "y = att @ v\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(att_unmask)\n",
    "plt.title(\"q @ k^T\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Input tokens\")\n",
    "plt.ylabel(\"Input tokens\")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(mask)\n",
    "plt.title(\"Mask\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Input tokens\")\n",
    "plt.ylabel(\"Input tokens\")\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(att_masked)\n",
    "plt.title(\"q @ k^T + mask\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Input tokens\")\n",
    "plt.ylabel(\"Input tokens\")\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(att)\n",
    "plt.title(\"softmax(q @ k^T + mask)\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Input tokens\")\n",
    "plt.ylabel(\"Input tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面请你实现完整的注意力模块 (2分)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0  # 保证隐层维度能均分到各个head上\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        # ======================================================================\n",
    "        # ===== 实现以下部分 (2分)\n",
    "        # =====     - Query、Key、Value矩阵\n",
    "        # =====     - 注意力掩码，保证当前token看不到后面的token\n",
    "        # ======================================================================\n",
    "        \n",
    "        # 每个头的key, query, value，写在一个Linear里面\n",
    "        self.c_attn = TODO\n",
    "        # 注意力掩码\n",
    "        self.register_buffer = TODO\n",
    "\n",
    "        # ======================================================================\n",
    "        # ===== 实现以上部分\n",
    "        # ======================================================================\n",
    "\n",
    "        # 合并各个head输出的线性层\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        # 一些正则化\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dim (n_embd)\n",
    "\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (y.transpose(1, 2).contiguous().view(B, T, C))  # (B, nh, T, hs) -> (B, T, C)\n",
    "        # 聚合各个head的输出\n",
    "        y = self.c_proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network (FFN) (2分)\n",
    "\n",
    "FFN层由两层线性层组成，中间的激活函数使用GELU。请你实现此部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T11:01:50.139407Z"
    }
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # ======================================================================\n",
    "        # ===== 实现以下部分 (2分)\n",
    "        # =====     - 第一个线性层使用`nn.Linear,维度为 n_embd * 4*n_nmbd\n",
    "        # =====     - 第二个线性层的维度为 4*n_embd * n_nmbd\n",
    "        # =====     - 激活函数使用`nn.GELU`\n",
    "        # ======================================================================\n",
    "        \n",
    "        self.c_fc = TODO\n",
    "        self.gelu = TODO\n",
    "        self.c_proj = TODO\n",
    "        # ======================================================================\n",
    "        # ===== 实现以上部分\n",
    "        # ======================================================================\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Decoder Block (2分)\n",
    "\n",
    "在上面的单元格中，你已经实现了Transformer的大部分组件，现在需要将各个组件拼装起来。\n",
    "\n",
    "下面请你实现Transformer的一个Decoder Block。一个Decoder Block由注意力、FFN、LayerNorm组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # ======================================================================\n",
    "        # ===== 实现以下部分 (2分)\n",
    "        # =====     - 模型输入需要分别经过：\n",
    "        # =====         1. LayerNorm\n",
    "        # =====         2. 多头自注意力\n",
    "        # =====         3. LayerNorm\n",
    "        # =====         4. FFN\n",
    "        # ======================================================================\n",
    "        self.ln_1 = TODO\n",
    "        self.attn = TODO\n",
    "        self.ln_2 = TODO\n",
    "        self.mlp = TODO\n",
    "        # ======================================================================\n",
    "        # ===== 实现以上部分\n",
    "        # ======================================================================\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GPT 模型 (2分)\n",
    "\n",
    "你已经实现了Decoder Block，只需要将其重复n次即可实现n层的Transformer主体部分。还需要添加输入embedding和输出lm head，才能构建一个完整的模型。\n",
    "\n",
    "下面请你实现完整的Transformer，将Embedding、Positional Encoding、各个Decoder Block、lm head组装起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        # ======================================================================\n",
    "        # ===== 实现以下部分 (2分)\n",
    "        # =====     - GPT模型需要包括：\n",
    "        # =====         1. wte：输入Token Embedding\n",
    "        # =====         2. wpe：位置编码\n",
    "        # =====         3. Dropout\n",
    "        # =====         4. n_layer个DecoderBlock块\n",
    "        # =====         5. LayerNorm\n",
    "        # =====         6. lm_head：输出线性层，相当于反向的输入Token Embedding。二者共享权重。\n",
    "        # ======================================================================\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = TODO\n",
    "                drop = TODO\n",
    "                h = TODO\n",
    "                ln_f = TODO\n",
    "            )\n",
    "        )\n",
    "        # ======================================================================\n",
    "        # ===== 实现以上部分\n",
    "        # ======================================================================\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # 初始化权重\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        _, T = idx.size()\n",
    "        assert (T <= self.config.block_size), f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # 同时需要计算loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 输入过长时，需要将它截断到block_size\n",
    "            idx_cond = (idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:])\n",
    "            # forward\n",
    "            logits, _ = self(idx_cond)\n",
    "            # 只保留最后一步的logits，同时用temperature放缩\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # 选择top-k个token\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # 使用softmax将logits转为概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 进行sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # 自回归，输出作为下一步的输入\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练 (0.5分)\n",
    "\n",
    "下面将用中文训练你实现的GPT模型，整个训练过程大约需要25分钟。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 一些超参数\n",
    "eval_interval = 250\n",
    "log_interval = 10\n",
    "eval_iters = 200\n",
    "batch_size = 20\n",
    "learning_rate = 1e-3\n",
    "max_iters = 5000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0\n",
    "decay_lr = True\n",
    "warmup_iters = 100\n",
    "lr_decay_iters = 5000\n",
    "min_lr = 1e-4\n",
    "\n",
    "# tensorboard支持\n",
    "writer = SummaryWriter(log_dir=f\"./tb/{int(time.time())}\")\n",
    "\n",
    "# 检查点保存\n",
    "out_dir = \"out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "checkpoint = None\n",
    "\n",
    "# 数据集加载\n",
    "data_dir = os.path.abspath(\"data\")\n",
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint32, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint32, mode=\"r\")\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - GPTConfig.block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i: i + GPTConfig.block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1: i + 1 + GPTConfig.block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "model_args = dict(\n",
    "    block_size = 256,\n",
    "    vocab_size = enc.n_vocab,\n",
    "    n_layer = 6,\n",
    "    n_head = 6,\n",
    "    n_embd = 384,\n",
    "    dropout = 0.2,\n",
    ")\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "model.to(device)\n",
    "\n",
    "# 初始化优化器\n",
    "param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "optim_groups = [\n",
    "    {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "    {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "]\n",
    "num_decay_params = sum(p.numel() for p in decay_params)\n",
    "num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(beta1, beta2), fused=True)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# 使用带有warmup的cosine学习率衰减\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "# 开始训练！\n",
    "best_val_loss = 1e9\n",
    "for iter_num in trange(max_iters, desc=\"Training\"):\n",
    "    # 设置当前学习率\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    writer.add_scalar(\"lr\", lr, iter_num)\n",
    "\n",
    "    # 进行验证，检查验证集loss，并保存检查点\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        writer.add_scalar(\"val/loss\", losses[\"val\"], iter_num)\n",
    "        if losses[\"val\"] < best_val_loss:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"model_args\": model_args,\n",
    "                    \"iter_num\": iter_num,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
    "\n",
    "    # forward和backward\n",
    "    X, Y = get_batch(\"train\")\n",
    "    logits, loss = model(X, Y)\n",
    "    loss.backward()\n",
    "    if grad_clip != 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 训练日志记录\n",
    "    if iter_num % log_interval == 0:\n",
    "        tqdm.write(f\"iter {iter_num}: loss {loss.item():.4f}\")\n",
    "    writer.add_scalar(\"train/loss\", loss.item(), iter_num)\n",
    "\n",
    "    iter_num += 1\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练结束！最终的train和valid的loss应该在3.5左右。\n",
    "\n",
    "你可以通过运行`tensorboard --logdir tb`启动Tensorboard，观察训练过程中的loss曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理 (0.5分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请你加载模型检查点，实际测试一下模型的生成能力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out\"\n",
    "start = \"我们这堂课要学习\"\n",
    "num_samples = 5\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "\n",
    "# 加载模型检查点\n",
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint[\"model\"]\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 加载tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "start_ids = encode(start)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "# 开始生成\n",
    "with torch.no_grad():\n",
    "    for k in range(num_samples):\n",
    "        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print(decode(y[0].tolist()))\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的句子应该类似于：\n",
    "\n",
    "> “我们这堂课要学习和意识，充分发挥了全球瘟究，不仅需要懂得深化的生活水平。十九大学时代，我国地区的企业从业界相应，以及学生的信赖，我们要深切实的和实施，强化自信、安全生动力。\n",
    "特别是信心、信心、不勇地为人民文明，不忘初心，是我们始终身务实的问题，需要专业教育更好的人，牢把优先发展、服务水平，使得我国的专业和服务生命，努力走出更多的基础，使客户到社会力量。”\n",
    "\n",
    "这种看似中文，但是内部没有逻辑的句子。这是因为训练的GPT模型参数量过小，导致其虽然可以正常执行语言模型的续写任务，但是对语言的理解能力仍有不足。下一步请你尝试修改`GPTConfig`中的各项参数，扩大模型规模，尝试训练更大的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
